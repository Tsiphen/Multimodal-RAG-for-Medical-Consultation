import os
import re
import torch
import numpy as np
import pickle
import math
from typing import List, Dict, Tuple, Optional, Set
from transformers import AutoTokenizer, AutoModel
from openai import OpenAI
import PyPDF2
from docx import Document
import faiss
from sklearn.metrics.pairwise import cosine_similarity
from dataclasses import dataclass
from collections import defaultdict, Counter

@dataclass
class ConfidenceMetrics:
    """ç½®ä¿¡åº¦æŒ‡æ ‡"""
    max_similarity: float      # æœ€å¤§ç›¸ä¼¼åº¦
    avg_similarity: float      # å¹³å‡ç›¸ä¼¼åº¦
    coverage_score: float      # è¦†ç›–åº¦è¯„åˆ†
    consistency_score: float   # ä¸€è‡´æ€§è¯„åˆ†
    medical_relevance: float   # åŒ»å­¦ç›¸å…³æ€§
    domain_alignment: float    # é¢†åŸŸå¯¹é½åº¦
    final_confidence: float    # æœ€ç»ˆç½®ä¿¡åº¦

@dataclass
class DomainContext:
    """åŒ»å­¦é¢†åŸŸä¸Šä¸‹æ–‡"""
    primary_domain: str           # ä¸»è¦é¢†åŸŸ
    domain_confidence: float      # é¢†åŸŸç½®ä¿¡åº¦
    entities: List[str]           # è¯†åˆ«çš„åŒ»å­¦å®ä½“
    expanded_terms: List[str]     # æ‰©å±•æœ¯è¯­
    domain_weights: Dict[str, float]  # é¢†åŸŸæƒé‡

class MedicalDomainClassifier:
    """åŒ»å­¦é¢†åŸŸåˆ†ç±»å™¨"""
    def __init__(self):
        # è¯¦ç»†çš„åŒ»å­¦ä¸“ä¸šé¢†åŸŸåˆ†ç±»
        self.domain_keywords = {
            'radiology': {
                'keywords': ['CT', 'MRI', 'æ ¸ç£', 'Xå…‰', 'Xçº¿', 'Bè¶…', 'è¶…å£°', 'å½±åƒ', 'æ”¾å°„',
                           'OCT', 'PET', 'SPECT', 'é€ å½±', 'DSA', 'è¡€ç®¡é€ å½±', 'ä»‹å…¥', 'å½±åƒå­¦',
                           'æ‰«æ', 'æ–­å±‚', 'é€è§†', 'èƒ¸ç‰‡', 'è…¹éƒ¨å¹³ç‰‡'],
                'weight': 1.0,
                'synonyms': ['å½±åƒç§‘', 'æ”¾å°„ç§‘', 'è¶…å£°ç§‘']
            },
            'internal_medicine': {
                'keywords': ['å†…ç§‘', 'é«˜è¡€å‹', 'ç³–å°¿ç—…', 'å¿ƒè„ç—…', 'è‚ºç—…', 'è‚ç—…', 'è‚¾ç—…',
                           'å†…åˆ†æ³Œ', 'æ¶ˆåŒ–', 'å‘¼å¸', 'å¾ªç¯', 'è¡€æ¶²', 'é£æ¹¿', 'å…ç–«',
                           'ä»£è°¢', 'å¿ƒè¡€ç®¡', 'èƒƒè‚ ', 'è‚èƒ†', 'è‚¾è„'],
                'weight': 1.0,
                'synonyms': ['å†…ç§‘å­¦', 'å¤§å†…ç§‘']
            },
            'surgery': {
                'keywords': ['æ‰‹æœ¯', 'å¤–ç§‘', 'åˆ‡é™¤', 'ç¼åˆ', 'éº»é†‰', 'æœ¯å‰', 'æœ¯å', 'æ‰‹æœ¯å®¤',
                           'å¼€åˆ€', 'å¾®åˆ›', 'è…¹è…”é•œ', 'ä»‹å…¥æ‰‹æœ¯', 'ç§»æ¤', 'é‡å»º', 'ä¿®å¤'],
                'weight': 1.0,
                'synonyms': ['å¤–ç§‘å­¦', 'æ‰‹æœ¯ç§‘']
            },
            'oncology': {
                'keywords': ['è‚¿ç˜¤', 'ç™Œç—‡', 'ç™Œ', 'æ¶æ€§', 'è‰¯æ€§', 'è½¬ç§»', 'åŒ–ç–—', 'æ”¾ç–—',
                           'é¶å‘æ²»ç–—', 'å…ç–«æ²»ç–—', 'è‚¿ç˜¤æ ‡å¿—ç‰©', 'ç—…ç†', 'æ´»æ£€', 'ç©¿åˆº',
                           'è‚ç™Œ', 'è‚ºç™Œ', 'èƒƒç™Œ', 'ä¹³è…ºç™Œ', 'ç»“è‚ ç™Œ', 'èƒ°è…ºç™Œ'],
                'weight': 1.2,
                'synonyms': ['è‚¿ç˜¤ç§‘', 'ç™Œç—‡ç§‘']
            },
            'gastroenterology': {
                'keywords': ['æ¶ˆåŒ–', 'èƒƒ', 'è‚ ', 'è‚', 'èƒ†', 'èƒ°è…º', 'è„¾', 'é£Ÿç®¡', 'åäºŒæŒ‡è‚ ',
                           'èƒƒé•œ', 'è‚ é•œ', 'ERCP', 'MRCP', 'èƒ†ç®¡', 'è‚ç¡¬åŒ–', 'è‚ç‚',
                           'èƒ†å›Š', 'èƒ†çŸ³', 'æ¶ˆåŒ–é“', 'è‚ èƒƒ', 'è‚èƒ†èƒ°'],
                'weight': 1.1,
                'synonyms': ['æ¶ˆåŒ–ç§‘', 'è‚èƒ†ç§‘', 'èƒƒè‚ ç§‘']
            },
            'cardiology': {
                'keywords': ['å¿ƒè„', 'å¿ƒè¡€ç®¡', 'å¿ƒç”µå›¾', 'å¿ƒè„ç—…', 'å† å¿ƒç—…', 'å¿ƒå¾‹',
                           'å¿ƒè‚Œ', 'å¿ƒç»ç—›', 'å¿ƒæ¢—', 'å¿ƒè¡°', 'é«˜è¡€å‹', 'åŠ¨è„‰ç¡¬åŒ–',
                           'å¿ƒè„å½©è¶…', 'å† è„‰é€ å½±', 'å¿ƒå¯¼ç®¡', 'ECG', 'EKG'],
                'weight': 1.0,
                'synonyms': ['å¿ƒå†…ç§‘', 'å¿ƒè¡€ç®¡ç§‘']
            },
            'emergency': {
                'keywords': ['æ€¥è¯Š', 'æ€¥æ•‘', 'æŠ¢æ•‘', 'å±é‡', 'é‡ç—‡', 'ICU', 'CCU',
                           'ä¼‘å…‹', 'æ˜è¿·', 'å¤–ä¼¤', 'ä¸­æ¯’', 'çª’æ¯', 'å¿ƒè·³éª¤åœ'],
                'weight': 1.3,
                'synonyms': ['æ€¥è¯Šç§‘', 'æ€¥æ•‘ç§‘', 'ICU']
            },
            'laboratory': {
                'keywords': ['åŒ–éªŒ', 'æ£€éªŒ', 'è¡€å¸¸è§„', 'å°¿å¸¸è§„', 'ç”ŸåŒ–', 'å…ç–«', 'å¾®ç”Ÿç‰©',
                           'è¡€ç³–', 'è‚åŠŸ', 'è‚¾åŠŸ', 'ç”µè§£è´¨', 'å‡è¡€', 'æ„ŸæŸ“æŒ‡æ ‡',
                           'è‚¿ç˜¤æ ‡å¿—ç‰©', 'æ¿€ç´ ', 'è¡€è„‚', 'è¡€æ°”åˆ†æ'],
                'weight': 1.0,
                'synonyms': ['æ£€éªŒç§‘', 'åŒ–éªŒç§‘']
            }
        }

        # åŒ»å­¦å®ä½“è¯†åˆ«æ¨¡å¼
        self.entity_patterns = {
            'disease': [r'(\w*ç—…\w*)', r'(\w*ç—‡\w*)', r'(\w*ç‚\w*)', r'(\w*ç™Œ\w*)', r'(\w*ç˜¤\w*)'],
            'symptom': [r'(ç–¼ç—›|å¤´ç—›|èƒ¸ç—›|è…¹ç—›|æ¶å¿ƒ|å‘•å|å‘çƒ­|å’³å—½|æ°”çŸ­|ä¹åŠ›)'],
            'examination': [r'(CT|MRI|Xå…‰|Bè¶…|èƒƒé•œ|è‚ é•œ|å¿ƒç”µå›¾|è¡€å¸¸è§„|å°¿å¸¸è§„|ç”ŸåŒ–æ£€æŸ¥)'],
            'treatment': [r'(æ‰‹æœ¯|åŒ–ç–—|æ”¾ç–—|è¯ç‰©æ²»ç–—|ä¿å®ˆæ²»ç–—|ä»‹å…¥æ²»ç–—)'],
            'anatomy': [r'(å¿ƒè„|è‚ºéƒ¨|è‚è„|è‚¾è„|èƒƒ|è‚ |èƒ†å›Š|èƒ°è…º|è„¾è„|ç”²çŠ¶è…º)']
        }

    def classify_domain(self, query: str) -> DomainContext:
        """åˆ†ç±»åŒ»å­¦é¢†åŸŸå¹¶æå–ä¸Šä¸‹æ–‡"""
        query_lower = query.lower()
        domain_scores = {}
        identified_entities = []

        # è®¡ç®—æ¯ä¸ªé¢†åŸŸçš„åŒ¹é…åˆ†æ•°
        for domain, info in self.domain_keywords.items():
            score = 0
            matched_terms = []

            for keyword in info['keywords']:
                if keyword.lower() in query_lower:
                    score += info['weight']
                    matched_terms.append(keyword)

            # åŒä¹‰è¯åŒ¹é…
            for synonym in info.get('synonyms', []):
                if synonym.lower() in query_lower:
                    score += info['weight'] * 0.8
                    matched_terms.append(synonym)

            if score > 0:
                domain_scores[domain] = score

        # æå–åŒ»å­¦å®ä½“
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, query, re.IGNORECASE)
                identified_entities.extend(matches)

        # ç¡®å®šä¸»è¦é¢†åŸŸ
        if domain_scores:
            primary_domain = max(domain_scores, key=domain_scores.get)
            domain_confidence = min(1.0, domain_scores[primary_domain] / 5.0)
        else:
            primary_domain = 'general'
            domain_confidence = 0.1

        # ç”Ÿæˆæ‰©å±•æœ¯è¯­
        expanded_terms = self._generate_expanded_terms(query, primary_domain, identified_entities)

        # è®¡ç®—é¢†åŸŸæƒé‡
        total_score = sum(domain_scores.values())
        domain_weights = {domain: score/total_score for domain, score in domain_scores.items()} if total_score > 0 else {}

        return DomainContext(
            primary_domain=primary_domain,
            domain_confidence=domain_confidence,
            entities=identified_entities,
            expanded_terms=expanded_terms,
            domain_weights=domain_weights
        )

    def _generate_expanded_terms(self, query: str, domain: str, entities: List[str]) -> List[str]:
        """ç”Ÿæˆæ‰©å±•æœ¯è¯­"""
        expanded = []

        # åŸºäºé¢†åŸŸçš„æœ¯è¯­æ‰©å±•
        if domain in self.domain_keywords:
            domain_info = self.domain_keywords[domain]
            expanded.extend(domain_info['keywords'][:5])
            expanded.extend(domain_info.get('synonyms', []))

        # åŸºäºå®ä½“çš„æ‰©å±•
        for entity in entities[:3]:
            if 'ç—…' in entity:
                expanded.append(entity.replace('ç—…', 'ç–¾ç—…'))
                expanded.append(entity + 'ç—‡çŠ¶')
                expanded.append(entity + 'æ²»ç–—')
            elif 'ç™Œ' in entity or 'ç˜¤' in entity:
                expanded.append(entity + 'è¯Šæ–­')
                expanded.append(entity + 'åˆ†æœŸ')
                expanded.append(entity + 'é¢„å')

        return list(set(expanded))  # å»é‡

class EnhancedSemanticChunker:
    """å¢å¼ºç‰ˆè¯­ä¹‰åˆ†å—å™¨ - åŒ»å­¦é¢†åŸŸæ„ŸçŸ¥"""
    def __init__(self, chunk_size: int = 500, overlap: int = 100):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.domain_classifier = MedicalDomainClassifier()

    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\u4e00-\u9fff\u3000-\u303f\uff00-\uffef.,;:!?()ï¼ˆï¼‰ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ]', '', text)
        return text.strip()

    def split_by_medical_sections(self, text: str) -> List[str]:
        """åŸºäºåŒ»å­¦æ–‡æ¡£ç»“æ„åˆ†å‰²"""
        section_patterns = [
            r'(?:ç—…å²|ç°ç—…å²|æ—¢å¾€å²)[:ï¼š]',
            r'(?:ä½“æ ¼æ£€æŸ¥|æŸ¥ä½“)[:ï¼š]',
            r'(?:è¾…åŠ©æ£€æŸ¥|å®éªŒå®¤æ£€æŸ¥)[:ï¼š]',
            r'(?:å½±åƒå­¦æ£€æŸ¥|å½±åƒå­¦è¡¨ç°)[:ï¼š]',
            r'(?:è¯Šæ–­|ä¸´åºŠè¯Šæ–­|æœ€ç»ˆè¯Šæ–­)[:ï¼š]',
            r'(?:æ²»ç–—|å¤„ç†|æ²»ç–—æ–¹æ¡ˆ)[:ï¼š]',
            r'(?:é¢„å|éšè®¿)[:ï¼š]',
        ]

        sections = []
        current_section = ""

        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue

            is_new_section = any(re.search(pattern, line, re.IGNORECASE) for pattern in section_patterns)

            if is_new_section and current_section:
                sections.append(current_section.strip())
                current_section = line
            else:
                current_section += " " + line if current_section else line

        if current_section.strip():
            sections.append(current_section.strip())

        return sections if sections else [text]

    def chunk_text_with_domain_awareness(self, text: str, document_domain: str = None) -> List[Dict]:
        """é¢†åŸŸæ„ŸçŸ¥çš„æ–‡æœ¬åˆ†å—"""
        text = self.clean_text(text)
        sections = self.split_by_medical_sections(text)

        chunks_with_metadata = []

        for section_idx, section in enumerate(sections):
            section_domain_context = self.domain_classifier.classify_domain(section)

            if len(section) <= self.chunk_size:
                chunks_with_metadata.append({
                    'text': section,
                    'domain': section_domain_context.primary_domain,
                    'domain_confidence': section_domain_context.domain_confidence,
                    'entities': section_domain_context.entities,
                    'section_type': self._identify_section_type(section),
                    'medical_density': self._calculate_medical_density(section)
                })
            else:
                sub_chunks = self._split_long_section(section)
                for chunk in sub_chunks:
                    chunk_domain_context = self.domain_classifier.classify_domain(chunk)
                    chunks_with_metadata.append({
                        'text': chunk,
                        'domain': chunk_domain_context.primary_domain,
                        'domain_confidence': chunk_domain_context.domain_confidence,
                        'entities': chunk_domain_context.entities,
                        'section_type': self._identify_section_type(chunk),
                        'medical_density': self._calculate_medical_density(chunk)
                    })

        return chunks_with_metadata

    def _split_long_section(self, section: str) -> List[str]:
        """åˆ†å‰²é•¿ç« èŠ‚"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', section)
        sentences = [s.strip() for s in sentences if s.strip()]

        chunks = []
        current_chunk = ""

        for sentence in sentences:
            if len(current_chunk) + len(sentence) > self.chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    words = current_chunk.split()
                    if len(words) > self.overlap:
                        overlap_text = ' '.join(words[-self.overlap:])
                        current_chunk = overlap_text + " " + sentence
                    else:
                        current_chunk = sentence
                else:
                    current_chunk = sentence
            else:
                current_chunk += " " + sentence if current_chunk else sentence

        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return chunks

    def _identify_section_type(self, text: str) -> str:
        """è¯†åˆ«ç« èŠ‚ç±»å‹"""
        text_lower = text.lower()

        if any(keyword in text_lower for keyword in ['ç—…å²', 'ç°ç—…å²', 'æ—¢å¾€å²', 'å®¶æ—å²']):
            return 'history'
        elif any(keyword in text_lower for keyword in ['ä½“æ£€', 'æŸ¥ä½“', 'ä½“æ ¼æ£€æŸ¥']):
            return 'physical_exam'
        elif any(keyword in text_lower for keyword in ['æ£€æŸ¥', 'åŒ–éªŒ', 'å½±åƒ', 'ct', 'mri']):
            return 'examination'
        elif any(keyword in text_lower for keyword in ['è¯Šæ–­', 'ç–¾ç—…', 'ç–‘è¯Š']):
            return 'diagnosis'
        elif any(keyword in text_lower for keyword in ['æ²»ç–—', 'æ‰‹æœ¯', 'è¯ç‰©', 'å¤„ç†']):
            return 'treatment'
        else:
            return 'general'

    def _calculate_medical_density(self, text: str) -> float:
        """è®¡ç®—åŒ»å­¦æœ¯è¯­å¯†åº¦"""
        words = re.findall(r'\w+', text.lower())
        if not words:
            return 0.0

        medical_count = 0
        medical_terms = set()

        for domain_info in self.domain_classifier.domain_keywords.values():
            medical_terms.update(keyword.lower() for keyword in domain_info['keywords'])

        for word in words:
            if word in medical_terms or any(med_term in word for med_term in medical_terms if len(med_term) >= 3):
                medical_count += 1

        return medical_count / len(words)

class DomainAwareVectorStore:
    """é¢†åŸŸæ„ŸçŸ¥å‘é‡å­˜å‚¨"""
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
        self.index = faiss.IndexFlatIP(dimension)
        self.chunks = []
        self.metadata = []
        self.domain_indices = defaultdict(list)  # æŒ‰é¢†åŸŸç´¢å¼•

    def add_documents(self, chunk_data: List[Dict], embeddings: np.ndarray, doc_metadata: List[Dict]):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        faiss.normalize_L2(embeddings)

        start_idx = len(self.chunks)
        self.index.add(embeddings.astype('float32'))

        for i, (chunk_info, doc_meta) in enumerate(zip(chunk_data, doc_metadata)):
            chunk_idx = start_idx + i

            combined_metadata = {
                **doc_meta,
                'domain': chunk_info['domain'],
                'domain_confidence': chunk_info['domain_confidence'],
                'entities': chunk_info['entities'],
                'section_type': chunk_info['section_type'],
                'medical_density': chunk_info['medical_density']
            }

            self.chunks.append(chunk_info['text'])
            self.metadata.append(combined_metadata)
            self.domain_indices[chunk_info['domain']].append(chunk_idx)

    def domain_aware_search(self, query_embedding: np.ndarray, domain_context: DomainContext,
                            k: int = 5, domain_boost: float = 1.5) -> List[Tuple[str, float, Dict]]:
        """é¢†åŸŸæ„ŸçŸ¥æœç´¢"""
        faiss.normalize_L2(query_embedding)

        scores, indices = self.index.search(query_embedding.astype('float32'), k * 3)

        candidates = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):
                candidates.append((
                    self.chunks[idx],
                    float(score),
                    self.metadata[idx],
                    idx
                ))

        reranked_results = self._rerank_by_domain(candidates, domain_context, domain_boost)
        return reranked_results[:k]

    def _rerank_by_domain(self, candidates: List[Tuple[str, float, Dict, int]],
                          domain_context: DomainContext, domain_boost: float) -> List[Tuple[str, float, Dict]]:
        """åŸºäºé¢†åŸŸä¸Šä¸‹æ–‡é‡æ’åº"""
        enhanced_candidates = []

        for chunk, base_score, metadata, idx in candidates:
            enhanced_score = base_score

            chunk_domain = metadata.get('domain', 'general')
            if chunk_domain == domain_context.primary_domain:
                enhanced_score *= domain_boost
            elif chunk_domain in domain_context.domain_weights:
                enhanced_score *= (1 + domain_context.domain_weights[chunk_domain] * 0.5)

            medical_density = metadata.get('medical_density', 0)
            enhanced_score *= (1 + medical_density * 0.3)

            chunk_entities = set(metadata.get('entities', []))
            query_entities = set(domain_context.entities)
            entity_overlap = len(chunk_entities.intersection(query_entities))
            if entity_overlap > 0:
                enhanced_score *= (1 + entity_overlap * 0.2)

            section_type = metadata.get('section_type', 'general')
            if self._is_relevant_section(section_type, domain_context):
                enhanced_score *= 1.2

            enhanced_candidates.append((chunk, enhanced_score, metadata))

        enhanced_candidates.sort(key=lambda x: x[1], reverse=True)
        return enhanced_candidates

    def _is_relevant_section(self, section_type: str, domain_context: DomainContext) -> bool:
        """åˆ¤æ–­ç« èŠ‚ç±»å‹æ˜¯å¦ä¸æŸ¥è¯¢é¢†åŸŸç›¸å…³"""
        relevance_map = {
            'radiology': ['examination', 'diagnosis'],
            'oncology': ['diagnosis', 'treatment', 'examination'],
            'surgery': ['treatment', 'physical_exam'],
            'laboratory': ['examination'],
            'emergency': ['history', 'physical_exam', 'diagnosis']
        }
        relevant_sections = relevance_map.get(domain_context.primary_domain, ['general'])
        return section_type in relevant_sections

    def save(self, path: str):
        """ä¿å­˜å‘é‡å­˜å‚¨"""
        data = {
            'chunks': self.chunks,
            'metadata': self.metadata,
            'domain_indices': dict(self.domain_indices)
        }

        faiss.write_index(self.index, f"{path}.faiss")
        with open(f"{path}.pkl", 'wb') as f:
            pickle.dump(data, f)

    def load(self, path: str):
        """åŠ è½½å‘é‡å­˜å‚¨"""
        self.index = faiss.read_index(f"{path}.faiss")
        with open(f"{path}.pkl", 'rb') as f:
            data = pickle.load(f)
            self.chunks = data['chunks']
            self.metadata = data['metadata']
            self.domain_indices = defaultdict(list, data.get('domain_indices', {}))

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ï¼Œæ”¯æŒPDFå’ŒDOCXæ ¼å¼"""
    def __init__(self):
        pass

    def read_pdf(self, file_path: str) -> str:
        """è¯»å–PDFæ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            print(f"è¯»å–PDFæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""

    def read_docx(self, file_path: str) -> str:
        """è¯»å–DOCXæ–‡ä»¶å†…å®¹"""
        try:
            doc = Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            print(f"è¯»å–DOCXæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""

    def load_documents(self, data_dir: str) -> Dict[str, str]:
        """åŠ è½½æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
        documents = {}
        for filename in os.listdir(data_dir):
            file_path = os.path.join(data_dir, filename)

            if filename.lower().endswith('.pdf'):
                content = self.read_pdf(file_path)
            elif filename.lower().endswith('.docx'):
                content = self.read_docx(file_path)
            else:
                continue

            if content.strip():
                documents[filename] = content
                print(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {filename}")

        return documents

class EmbeddingModel:
    """åµŒå…¥æ¨¡å‹"""
    def __init__(self, model_path: str = None):
        # ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œé¿å…ç¡¬ç¼–ç ä¸ªäººè·¯å¾„
        model_path = model_path or os.getenv(
            "EMBED_MODEL_PATH",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModel.from_pretrained(model_path)
        self.model.eval()

    def mean_pooling(self, model_output, attention_mask):
        """å¹³å‡æ± åŒ–"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def encode(self, texts: List[str], batch_size: int = 16) -> np.ndarray:
        """å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡"""
        all_embeddings = []
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            encoded_input = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            )
            with torch.no_grad():
                model_output = self.model(**encoded_input)
            embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])
            all_embeddings.append(embeddings.numpy())
        return np.vstack(all_embeddings)

class LLMClient:
    """LLMå®¢æˆ·ç«¯"""
    def __init__(self):
        # æ”¹ä¸ºè¯»å–ç¯å¢ƒå˜é‡ï¼Œé¿å…æš´éœ²ä¸ªäººä¿¡æ¯
        self.client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL")  # å¯ä¸ºç©ºï¼Œèµ°å®˜æ–¹é»˜è®¤
        )
        # å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–æ¨¡å‹å
        self.model_name = os.getenv("LLM_MODEL", "deepseek-ai/DeepSeek-R1")

    def generate_response(self, query: str, context: str = "", domain_context: DomainContext = None) -> str:
        """ç”Ÿæˆå›ç­”"""
        if context.strip():
            domain_info = ""
            if domain_context:
                ents_preview = ', '.join(domain_context.entities[:5]) if domain_context.entities else ''
                domain_info = f"\næ£€æµ‹åˆ°çš„åŒ»å­¦é¢†åŸŸ: {domain_context.primary_domain}\nç›¸å…³åŒ»å­¦å®ä½“: {ents_preview}\n"

            prompt = f"""è¯·ç»“åˆä»¥ä¸‹æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ä¿¡æ¯å’Œä½ è‡ªèº«çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå…¨é¢å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚{domain_info}

            æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š
            {context}
            
            é—®é¢˜ï¼š{query}
            
            è¯·åŸºäºä¸Šè¿°æ–‡æ¡£ä¿¡æ¯å’Œä½ çš„ä¸“ä¸šçŸ¥è¯†æä¾›å‡†ç¡®ã€å…¨é¢çš„å›ç­”ï¼š"""
        else:
            prompt = f"""è¯·åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
            
            é—®é¢˜ï¼š{query}
            
            å›ç­”ï¼š"""

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦åŠ©æ‰‹ã€‚è¯·ç»“åˆæä¾›çš„æ–‡æ¡£ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰å’Œä½ è‡ªèº«çš„åŒ»å­¦çŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€å…¨é¢ä¸”æœ‰ç”¨çš„å›ç­”ã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                temperature=0.7,
                max_tokens=1500
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}"

class EnhancedRAGSystem:
    """å¢å¼ºç‰ˆRAGç³»ç»Ÿ - é¢†åŸŸæ„ŸçŸ¥æ£€ç´¢"""
    def __init__(self, model_path: str = None):
        self.doc_processor = DocumentProcessor()
        self.chunker = EnhancedSemanticChunker(chunk_size=500, overlap=100)
        self.embedding_model = EmbeddingModel(model_path)
        self.vector_store = DomainAwareVectorStore()
        self.llm_client = LLMClient()
        self.domain_classifier = MedicalDomainClassifier()
        self.is_built = False

        # è°ƒæ•´åçš„é˜ˆå€¼å‚æ•°
        self.similarity_threshold = 0.4
        self.coverage_threshold = 0.3
        self.consistency_threshold = 0.4
        self.medical_threshold = 0.3
        self.domain_threshold = 0.25     # æ–°å¢ï¼šé¢†åŸŸå¯¹é½é˜ˆå€¼
        self.confidence_threshold = 0.5

    def build_index(self, data_dir: str, save_path: str = "enhanced_rag_index"):
        """æ„å»ºå¢å¼ºç´¢å¼•"""
        print("æ­£åœ¨åŠ è½½æ–‡æ¡£...")
        documents = self.doc_processor.load_documents(data_dir)

        if not documents:
            print("æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£")
            return

        print("æ­£åœ¨è¿›è¡Œé¢†åŸŸæ„ŸçŸ¥åˆ†å‰²...")
        all_chunk_data = []
        all_metadata = []

        for doc_name, content in documents.items():
            print(f"å¤„ç†æ–‡æ¡£: {doc_name}")
            chunk_data = self.chunker.chunk_text_with_domain_awareness(content)
            all_chunk_data.extend(chunk_data)
            for chunk_info in chunk_data:
                all_metadata.append({
                    'document': doc_name,
                    'chunk_length': len(chunk_info['text'])
                })

        print(f"æ€»å…±åˆ†å‰²å‡º {len(all_chunk_data)} ä¸ªé¢†åŸŸæ„ŸçŸ¥æ–‡æœ¬å—")

        domain_stats = Counter(chunk['domain'] for chunk in all_chunk_data)
        print("é¢†åŸŸåˆ†å¸ƒ:")
        for domain, count in domain_stats.most_common():
            print(f"  {domain}: {count}")

        print("æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡...")
        chunk_texts = [chunk['text'] for chunk in all_chunk_data]
        embeddings = self.embedding_model.encode(chunk_texts)

        print("æ­£åœ¨æ„å»ºé¢†åŸŸæ„ŸçŸ¥å‘é‡ç´¢å¼•...")
        self.vector_store.add_documents(all_chunk_data, embeddings, all_metadata)

        print("æ­£åœ¨ä¿å­˜ç´¢å¼•...")
        self.vector_store.save(save_path)

        self.is_built = True
        print("å¢å¼ºç´¢å¼•æ„å»ºå®Œæˆï¼")

    def load_index(self, save_path: str = "enhanced_rag_index"):
        """åŠ è½½ç´¢å¼•"""
        try:
            self.vector_store.load(save_path)
            self.is_built = True
            print("å¢å¼ºç´¢å¼•åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")

    def _calculate_enhanced_confidence_metrics(self, query: str, search_results: List[Tuple[str, float, Dict]],
                                               domain_context: DomainContext) -> ConfidenceMetrics:
        """è®¡ç®—å¢å¼ºçš„ç½®ä¿¡åº¦æŒ‡æ ‡"""
        if not search_results:
            return ConfidenceMetrics(0, 0, 0, 0, 0, 0, 0)

        chunks = [result[0] for result in search_results]
        similarities = [result[1] for result in search_results]

        max_sim = max(similarities)
        avg_sim = sum(similarities) / len(similarities)

        # è¦†ç›–åº¦è¯„åˆ†
        coverage = 0.0
        for i, sim in enumerate(similarities[:5]):
            position_weight = 1.0 / math.log2(i + 2)
            indicator = 1.0 if sim >= self.similarity_threshold else 0.0
            coverage += position_weight * indicator
        max_coverage = sum(1.0 / math.log2(i + 2) for i in range(min(len(similarities), 5)))
        coverage_score = coverage / max_coverage if max_coverage > 0 else 0.0

        # ä¸€è‡´æ€§è¯„åˆ†
        if len(similarities) <= 1:
            consistency = 1.0
        else:
            variance = sum((sim - avg_sim) ** 2 for sim in similarities) / len(similarities)
            consistency = 1.0 / (1.0 + variance)

        # åŒ»å­¦ç›¸å…³æ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        medical_relevance = domain_context.domain_confidence

        # æ–°å¢ï¼šé¢†åŸŸå¯¹é½åº¦è¯„åˆ†
        domain_alignment = self._calculate_domain_alignment(search_results, domain_context)

        # æƒé‡è°ƒæ•´ï¼šåŠ å…¥é¢†åŸŸå¯¹é½åº¦
        w1, w2, w3, w4, w5 = 0.35, 0.25, 0.15, 0.15, 0.1
        final_confidence = (w1 * max_sim + w2 * coverage_score + w3 * consistency +
                            w4 * medical_relevance + w5 * domain_alignment)

        return ConfidenceMetrics(
            max_similarity=max_sim,
            avg_similarity=avg_sim,
            coverage_score=coverage_score,
            consistency_score=consistency,
            medical_relevance=medical_relevance,
            domain_alignment=domain_alignment,
            final_confidence=final_confidence
        )

    def _calculate_domain_alignment(self, search_results: List[Tuple[str, float, Dict]],
                                    domain_context: DomainContext) -> float:
        """è®¡ç®—é¢†åŸŸå¯¹é½åº¦"""
        if not search_results:
            return 0.0

        alignment_score = 0.0
        for chunk, score, metadata in search_results:
            chunk_domain = metadata.get('domain', 'general')
            domain_confidence = metadata.get('domain_confidence', 0.0)

            if chunk_domain == domain_context.primary_domain:
                alignment_score += score * domain_confidence * 1.0
            elif chunk_domain in domain_context.domain_weights:
                weight = domain_context.domain_weights[chunk_domain]
                alignment_score += score * domain_confidence * weight * 0.7
            else:
                alignment_score += score * domain_confidence * 0.3

        return min(1.0, alignment_score / len(search_results))

    def _should_reject_enhanced(self, confidence_metrics: ConfidenceMetrics) -> Tuple[bool, str]:
        """å¢å¼ºçš„æ‹’ç»åˆ¤æ–­"""
        if confidence_metrics.final_confidence < self.confidence_threshold:
            return True, f"æ•´ä½“ç½®ä¿¡åº¦è¿‡ä½ ({confidence_metrics.final_confidence:.3f} < {self.confidence_threshold})"

        if confidence_metrics.max_similarity < self.similarity_threshold:
            return True, f"æ–‡æ¡£ç›¸ä¼¼åº¦è¿‡ä½ ({confidence_metrics.max_similarity:.3f} < {self.similarity_threshold})"

        if confidence_metrics.domain_alignment < self.domain_threshold:
            return True, f"é¢†åŸŸå¯¹é½åº¦è¿‡ä½ ({confidence_metrics.domain_alignment:.3f} < {self.domain_threshold})"

        return False, "é€šè¿‡å¢å¼ºç½®ä¿¡åº¦æ£€æŸ¥"

    def query(self, question: str, top_k: int = 3) -> str:
        """å¢å¼ºæŸ¥è¯¢ç³»ç»Ÿ"""
        if not self.is_built:
            return "è¯·å…ˆæ„å»ºæˆ–åŠ è½½ç´¢å¼•"

        print(f"æŸ¥è¯¢: {question}")

        domain_context = self.domain_classifier.classify_domain(question)
        print(f"\n=== é¢†åŸŸåˆ†æ ===")
        print(f"ä¸»è¦é¢†åŸŸ: {domain_context.primary_domain}")
        print(f"é¢†åŸŸç½®ä¿¡åº¦: {domain_context.domain_confidence:.3f}")
        print(f"è¯†åˆ«å®ä½“: {domain_context.entities}")
        print(f"æ‰©å±•æœ¯è¯­: {domain_context.expanded_terms[:5]}")

        enhanced_query = question + " " + " ".join(domain_context.expanded_terms[:3])
        query_embedding = self.embedding_model.encode([enhanced_query])

        results = self.vector_store.domain_aware_search(
            query_embedding,
            domain_context,
            k=max(top_k, 5),
            domain_boost=1.5
        )

        confidence_metrics = self._calculate_enhanced_confidence_metrics(question, results, domain_context)

        print(f"\n=== å¢å¼ºç½®ä¿¡åº¦åˆ†æ ===")
        print(f"æœ€å¤§ç›¸ä¼¼åº¦: {confidence_metrics.max_similarity:.3f}")
        print(f"å¹³å‡ç›¸ä¼¼åº¦: {confidence_metrics.avg_similarity:.3f}")
        print(f"è¦†ç›–åº¦è¯„åˆ†: {confidence_metrics.coverage_score:.3f}")
        print(f"ä¸€è‡´æ€§è¯„åˆ†: {confidence_metrics.consistency_score:.3f}")
        print(f"åŒ»å­¦ç›¸å…³æ€§: {confidence_metrics.medical_relevance:.3f}")
        print(f"é¢†åŸŸå¯¹é½åº¦: {confidence_metrics.domain_alignment:.3f}")
        print(f"æœ€ç»ˆç½®ä¿¡åº¦: {confidence_metrics.final_confidence:.3f}")

        should_reject, reason = self._should_reject_enhanced(confidence_metrics)

        if should_reject:
            print(f"\nâŒ æ‹’ç»å›ç­”: {reason}")
            return f"""æŠ±æ­‰ï¼ŒåŸºäºå½“å‰æ–‡æ¡£åº“åœ¨{domain_context.primary_domain}é¢†åŸŸçš„å†…å®¹ï¼Œæˆ‘æ— æ³•ä¸ºæ‚¨çš„é—®é¢˜æä¾›å¯é çš„åŒ»å­¦å»ºè®®ã€‚

æ‹’ç»åŸå› ï¼š{reason}

å»ºè®®æ‚¨å’¨è¯¢ä¸“ä¸šçš„{domain_context.primary_domain}ç§‘åŒ»ç”Ÿã€‚"""

        else:
            print(f"\nâœ… é€šè¿‡å¢å¼ºæ£€æŸ¥: {reason}")

            relevant_results = [(chunk, score, metadata) for chunk, score, metadata in results[:top_k]
                                if score >= self.similarity_threshold * 0.8]

            if relevant_results:
                context_parts = []
                for i, (chunk, score, metadata) in enumerate(relevant_results):
                    domain_info = f"[{metadata.get('domain', 'general')}é¢†åŸŸ]"
                    context_parts.append(
                        f"æ–‡æ¡£ç‰‡æ®µ{i+1}{domain_info}ï¼ˆæ¥æºï¼š{metadata['document']}ï¼Œç›¸ä¼¼åº¦ï¼š{score:.3f}ï¼‰ï¼š\n{chunk}\n"
                    )

                context = "\n".join(context_parts)

                print("æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š")
                for i, (chunk, score, metadata) in enumerate(relevant_results):
                    print(f"ç‰‡æ®µ{i+1} (é¢†åŸŸ: {metadata.get('domain')}, ç›¸ä¼¼åº¦: {score:.3f}):")
                    print(f"{chunk[:200]}...")
                    print("-" * 50)

                response = self.llm_client.generate_response(question, context, domain_context)
                confidence_note = f"\n\n[ç³»ç»Ÿç½®ä¿¡åº¦: {confidence_metrics.final_confidence:.3f}/1.0 - åŸºäº{domain_context.primary_domain}é¢†åŸŸçš„å¯é å›ç­”]"
                return response + confidence_note

            else:
                print("æœªæ£€ç´¢åˆ°é«˜ç›¸å…³æ€§çš„æ–‡æ¡£ç‰‡æ®µ...")
                response = self.llm_client.generate_response(question, "", domain_context)
                confidence_note = f"\n\n[ç³»ç»Ÿç½®ä¿¡åº¦: {confidence_metrics.final_confidence:.3f}/1.0 - åŸºäºæ¨¡å‹çŸ¥è¯†çš„{domain_context.primary_domain}é¢†åŸŸå›ç­”]"
                return response + confidence_note

def main():
    """ä¸»å‡½æ•°"""
    rag = EnhancedRAGSystem()

    data_dir = "./data"

    if os.path.exists("enhanced_rag_index.faiss") and os.path.exists("enhanced_rag_index.pkl"):
        print("å‘ç°å·²å­˜åœ¨çš„å¢å¼ºç´¢å¼•ï¼Œæ­£åœ¨åŠ è½½...")
        rag.load_index()
    else:
        print("æœªå‘ç°å¢å¼ºç´¢å¼•ï¼Œå¼€å§‹æ„å»ºæ–°çš„é¢†åŸŸæ„ŸçŸ¥ç´¢å¼•...")
        rag.build_index(data_dir)

    print("\n" + "="*80)
    print("ğŸš€ é¢†åŸŸæ„ŸçŸ¥å¢å¼ºåŒ»å­¦RAGç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼")
    print("ğŸ“š ç‰¹æ€§ï¼š")
    print("   â€¢ åŒ»å­¦é¢†åŸŸè‡ªåŠ¨è¯†åˆ«å’Œåˆ†ç±»")
    print("   â€¢ é¢†åŸŸæ„ŸçŸ¥çš„æ™ºèƒ½æ£€ç´¢")
    print("   â€¢ å¤šå±‚æ¬¡ç½®ä¿¡åº¦è¯„ä¼°")
    print("   â€¢ ä¸“ä¸šé¢†åŸŸè‡ªé€‚åº”æƒé‡è°ƒæ•´")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("="*80)

    while True:
        question = input("\nğŸ” è¯·è¾“å…¥æ‚¨çš„åŒ»å­¦é—®é¢˜: ").strip()

        if question.lower() in ['quit', 'exit', 'é€€å‡º']:
            print("ğŸ‘‹ å†è§ï¼")
            break

        if not question:
            continue

        print("\nğŸ”¬ æ­£åœ¨è¿›è¡Œé¢†åŸŸæ„ŸçŸ¥åˆ†æå’Œæ£€ç´¢...")
        answer = rag.query(question)

        print(f"\nğŸ’¡ å›ç­”ï¼š\n{answer}")
        print("\n" + "-"*80)

if __name__ == "__main__":
    main()
